{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62564764",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_PATH = '../../data/documents_functions_data/document_function_train.csv'\n",
    "TEST_PATH = '../../data/documents_functions_data/document_function_test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf62b317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.express as px\n",
    "import spacy\n",
    "import plotly.express as px\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import plot_confusion_matrix, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from flair.models import TARSClassifier\n",
    "from flair.data import Sentence\n",
    "from flair.datasets import SentenceDataset\n",
    "from flair.trainers import ModelTrainer\n",
    "from flair.data import Corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa476a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(TRAIN_PATH)\n",
    "df_test = pd.read_csv(TEST_PATH)\n",
    "\n",
    "X_train = df.title\n",
    "y_train = df.documentFunction\n",
    "X_test = df_test.title\n",
    "y_test = df_test.documentFunction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cba5b89",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.histogram(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11d59faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "991daecf",
   "metadata": {},
   "source": [
    "## Baseline: argmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8688bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(df.documentFunction, ['strategy']*len(df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b550054",
   "metadata": {},
   "source": [
    "## Baseline: logistic regression on BoW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4609431e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_lemmas(df):\n",
    "    lemmas = df.title.apply(nlp).apply(lambda d: [(t.lemma_).lower() for t in d if not t.is_stop])\n",
    "    lemmas_joined = lemmas.apply(lambda t: ' '.join(t))\n",
    "    return lemmas_joined\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "X = prep_lemmas(df)\n",
    "classifier = LogisticRegression(penalty=\"l2\", multi_class=\"multinomial\")\n",
    "\n",
    "pipeline = Pipeline(\n",
    "    [\n",
    "        (\"preprocessing\", CountVectorizer()),\n",
    "        (\"classifier\", LogisticRegression(penalty=\"l2\", multi_class=\"multinomial\")),\n",
    "    ]\n",
    ")\n",
    "\n",
    "pipeline = pipeline.fit(X, df.documentFunction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842411ee",
   "metadata": {},
   "source": [
    "#### Train acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a72f6ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = pipeline.predict(X)\n",
    "accuracy_score(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31fada4d",
   "metadata": {},
   "source": [
    "#### Test acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ade0c25",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = prep_lemmas(df_test)\n",
    "\n",
    "y_pred = pipeline.predict(X)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0efeb8c3",
   "metadata": {},
   "source": [
    "## TARS: zero shot (TODO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40251c76",
   "metadata": {},
   "outputs": [],
   "source": [
    "tars = TARSClassifier.load('tars-base')\n",
    "classes = list(df_test.documentFunction.drop_duplicates())\n",
    "def predict_zero_shot(titles):\n",
    "    prepared = titles.apply(Sentence)\n",
    "    prepared.apply()\n",
    "s = Sentence(df.title[1])\n",
    "tars.predict_zero_shot(s, classes)\n",
    "#TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3862be",
   "metadata": {},
   "source": [
    "## TARS: few shot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ef6764",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_set(sentences: list, labels: list, label_name: str):\n",
    "    dataset = SentenceDataset(\n",
    "        [\n",
    "            Sentence(sent).add_label(label_name, label)\n",
    "            for sent, label in zip(sentences, labels)\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "train_set = create_set(X_train, y_train, \"Document function\")\n",
    "test_set = create_set(X_test, y_test, \"Document function\")\n",
    "corpus = Corpus(train=train_set, test=test_set)\n",
    "label_type = 'Document function'\n",
    "label_dict = corpus.make_label_dictionary(label_type=label_type)\n",
    "\n",
    "tars.add_and_switch_to_new_task(\"Document function classification\",label_dictionary=label_dict,label_type=label_type)\n",
    "trainer = ModelTrainer(tars, corpus)\n",
    "trainer.train(base_path='resources/taggers/policy_doc_class', # path to store the model artifacts\n",
    "               learning_rate=0.02, # use very small learning rate\n",
    "               mini_batch_size=16, # small mini-batch size since corpus is tiny\n",
    "              max_epochs=10, # terminate after 10 epochs\n",
    "              train_with_dev=True,\n",
    "              )\n",
    "\n",
    "trained = TARSClassifier.load('resources/taggers/policy_doc_class/final-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21e44099",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sents = X_test.apply(Sentence)\n",
    "trained.predict(list(test_sents))\n",
    "predictions = test_sents.apply(lambda x: x.labels[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228941dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = y_test.unique()\n",
    "ConfusionMatrixDisplay(\n",
    "    confusion_matrix(y_test, predictions, labels=labels), display_labels=labels\n",
    ").plot(xticks_rotation=\"vertical\", cmap=plt.cm.Blues, colorbar=False)\n",
    "plt.tight_layout()\n",
    "plt.savefig('confusion_matrix.pdf')#, dpi=200)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1028dfd3",
   "metadata": {},
   "source": [
    "## Bigger labels for TARS (full sentences as labels) (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea61aab",
   "metadata": {},
   "source": [
    "## Using first sentences from documents as training data (TODO)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efdcf3cb",
   "metadata": {},
   "source": [
    "## RoBerta Baseline (TODO)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
